{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchsummary import summary \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, RandomHorizontalFlip, RandomCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "patch_size= 16\n",
    "latent_size= 768\n",
    "n_channels= 3\n",
    "num_heads=12\n",
    "num_encoders= 12\n",
    "dropout= 0.1\n",
    "num_classes = 10\n",
    "size= 224\n",
    "\n",
    "epochs = 10\n",
    "base_lr= 10e-3\n",
    "weight_decay= 0.03\n",
    "batch_size= 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=patch_size, n_channels= n_channels, device=device, latent_size=latent_size, batch_size=batch_size):\n",
    "        super(InputEmbedding,self).__init__()\n",
    "        self.latent_size= latent_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.device= device\n",
    "        self.batch_size= batch_size\n",
    "        self.input_size= self.patch_size*self.patch_size*self.n_channels\n",
    "\n",
    "        self.linearProjection = nn.Linear(self.input_size, self.latent_size)\n",
    "\n",
    "        self.class_token = nn.Parameter(torch.randn(self.batch_size, 1, self.latent_size)).to(self.device)\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(self.batch_size, 1, self.latent_size)).to(self.device)\n",
    "\n",
    "    def forward(self,input_data):\n",
    "        input_data= input_data.to(self.device)\n",
    "\n",
    "        patches = einops.rearrange(\n",
    "            input_data, 'b c (h h1) (w w1) -> b (h w) (h1 w1 c)', h1=self.patch_size, w1= self.patch_size)\n",
    "        \n",
    "        #print(input_data.size())\n",
    "        #print(patches.size())\n",
    "\n",
    "        linear_projection= self.linearProjection(patches).to(self.device)\n",
    "        b, n, _ = linear_projection.shape\n",
    "\n",
    "        linear_projection = torch.cat((self.class_token, linear_projection), dim=1)\n",
    "        pos_embed = einops.repeat(self.pos_embedding, 'b 1 d -> b m d', m= n+1)\n",
    "        \n",
    "        linear_projection += pos_embed\n",
    "\n",
    "        return linear_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "test_input= torch.randn((1,3,224,224))\n",
    "test_class = InputEmbedding().to(device)\n",
    "embed_test = test_class(test_input)\n",
    "print(test_input.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, latent_size=latent_size, num_heads= num_heads, device=device, dropout= dropout):\n",
    "        super(EncoderBlock,self).__init__()\n",
    "\n",
    "        self.latent_size= latent_size\n",
    "        self.num_heads= num_heads\n",
    "        self.device = device\n",
    "        self.dropout= dropout\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.latent_size)\n",
    "\n",
    "        self.multihead = nn.MultiheadAttention(\n",
    "            self.latent_size, self.num_heads, dropout = self.dropout\n",
    "        )\n",
    "        \n",
    "        self.enc_MLP= nn.Sequential(\n",
    "            nn.Linear(self.latent_size, self.latent_size*4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.latent_size*4, self.latent_size),\n",
    "            nn.Dropout(self.dropout),\n",
    "        )\n",
    "        \n",
    "    def forward(self, embedded_patches):\n",
    "        firstnorm_out= self.norm(embedded_patches)\n",
    "        attention_out = self.multihead(firstnorm_out, firstnorm_out, firstnorm_out)[0]\n",
    "\n",
    "        first_added= attention_out + embedded_patches\n",
    "\n",
    "        secondnorm_out = self.norm(first_added)\n",
    "        ff_out= self.enc_MLP(secondnorm_out)\n",
    "\n",
    "\n",
    "        return ff_out + first_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1638,  0.0723, -1.9514,  ..., -0.2565, -1.0283, -2.3148],\n",
       "         [-1.2423, -0.2888, -1.0887,  ..., -0.9608, -0.0467, -0.7384],\n",
       "         [-1.5194,  0.1549, -1.0216,  ...,  0.1413, -0.2636, -0.2604],\n",
       "         ...,\n",
       "         [-2.1037, -0.8994, -1.4926,  ...,  0.0334, -0.5196, -0.4258],\n",
       "         [-0.4757, -0.0628, -2.0112,  ..., -0.0492, -1.0608, -1.0428],\n",
       "         [-1.9993, -0.7510, -0.8350,  ...,  1.0831,  0.8381,  0.4590]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoder = EncoderBlock().to(device)\n",
    "test_encoder(embed_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,input_size= size, num_encoders=num_encoders, latent_size= latent_size, device= device, num_classes= num_classes, dropout=dropout):\n",
    "        super(VisionTransformer,self).__init__()\n",
    "        self.num_encoders= num_encoders\n",
    "        self.latent_size= latent_size\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout= dropout\n",
    "        self.input_size= input_size\n",
    "\n",
    "        self.embedding = InputEmbedding()\n",
    "\n",
    "        self.encStack = nn.ModuleList([EncoderBlock() for i in range(self.num_encoders)])\n",
    "\n",
    "        self.MLP_head= nn.Sequential(\n",
    "            nn.LayerNorm(self.latent_size),\n",
    "            nn.Linear(self.latent_size, self.latent_size),\n",
    "            nn.Linear(self.latent_size, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, test_input):\n",
    "        enc_output = self.embedding(test_input)\n",
    "\n",
    "        for enc_layer in self.encStack:\n",
    "            enc_output = enc_layer(enc_output)\n",
    "\n",
    "        cls_token_embed= enc_output[:, 0]\n",
    "\n",
    "        return self.MLP_head(cls_token_embed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2986,  0.1984, -0.3553, -0.0588, -0.1278, -0.2364,  0.5300,  0.4804,\n",
      "          0.1486,  0.1570]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "model= VisionTransformer().to(device)\n",
    "vit_output= model(test_input)\n",
    "print(vit_output)\n",
    "print(vit_output.size())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
